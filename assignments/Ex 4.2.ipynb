{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*a) Generate a simulated data set as follows:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "srand(1)\n",
    "x = randn(100) \n",
    "y = x - 2x.^2 + randn(100)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In this data set, what is n and what is p? Write out the model used to generate the data in equation form*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this regression model $n = 100$ which is the length of the variables $x$ and $y$. It seems reasonable to choose $p=2$, since $y$ is generated with from the two predictors $x$ and $x^2$, plus a  random error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(c) Do 5-fold cross-validation to fit polynomial models using least squares, we compute the generalization error*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Create the folds\n",
    "\n",
    "We will create 5 random equally sized subsamples, each one will be used as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Array{Int64,1},1}:\n",
       " [45, 92, 34, 86, 76, 30, 94, 88, 97, 79, 66, 100, 21, 80, 4, 72, 59, 42, 69, 31]\n",
       " [14, 85, 5, 19, 74, 6, 48, 89, 60, 75, 87, 83, 81, 15, 71, 23, 27, 40, 41, 29]  \n",
       " [82, 49, 22, 44, 17, 9, 73, 13, 28, 56, 11, 1, 7, 36, 20, 33, 47, 35, 68, 63]   \n",
       " [84, 95, 99, 98, 77, 61, 24, 38, 53, 16, 62, 52, 43, 3, 54, 91, 25, 39, 26, 32] \n",
       " [65, 93, 2, 70, 67, 46, 64, 90, 57, 12, 78, 10, 37, 18, 51, 55, 96, 8, 50, 58]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100\n",
    "k = 5\n",
    "s = convert(Integer, n / k)\n",
    "seq = randperm(n)\n",
    "fold = [seq[((i-1)*s + 1):(i*s)] for i in 1:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: fit models with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design matrix with polynomial entries\n",
    "X = [ones(length(y)) x x.^2 x.^3 x.^4]   \n",
    "# store space for generalization error\n",
    "gen_error = Array{Float64}(4, k)\n",
    "train_error = Array{Float64}(4, k)\n",
    "βhat_all = [[] for i in 1:k]\n",
    "# k-fold cross validation\n",
    "for i in 1:k # outer loop is the cross-validation loop\n",
    "    # test and train indices\n",
    "    test_idx = fold[i]\n",
    "    train_idx = [l for l in 1:n if l ∉ fold[i]]\n",
    "    # y data\n",
    "    y_train = y[train_idx]\n",
    "    y_test = y[test_idx]\n",
    "    # fit model\n",
    "    for deg in 1:4 # inner loop fits different polynomial models\n",
    "        # X data\n",
    "        X_train = X[train_idx, 1:(1 + deg)]\n",
    "        X_test = X[test_idx, 1:(1 + deg)]\n",
    "        # regression\n",
    "        βhat_train = Symmetric(X_train' * X_train) \\ (X_train' * y_train)\n",
    "        push!(βhat_all[i], βhat_train)\n",
    "        yhat_test = X_test * βhat_train\n",
    "        yhat_train = X_train * βhat_train            \n",
    "        gen_error[deg, i] = mean((yhat_test - y_test).^2) # average cross validation error \n",
    "        train_error[deg, i] = mean((yhat_train - y_train).^2) # average cross validation error \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now print the mean squared error (MSE) per degree of fitted polynomial. We see that on average higher degrees performed better than degree one. Unfortunately, with my data, not even cross validation can detect that the true degree is two, since higher degrees perform as well. I was expecting cross validation to reduce overfitting, but it did not. However, we do see that the performance is much worst in the test set that in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = mapslices(mean, train_error, 2)\n",
    "mse_test = mapslices(mean, gen_error, 2)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree: 1, MSE Train: 6.45, MSE Test: 7.52, SE test: [9.03, 4.0, 7.25, 5.21, 12.13] \n",
      "Degree: 2, MSE Train: 0.99, MSE Test: 1.08, SE test: [1.44, 1.11, 1.04, 1.02, 0.81] \n",
      "Degree: 3, MSE Train: 0.97, MSE Test: 1.07, SE test: [1.4, 1.05, 1.06, 1.1, 0.72] \n",
      "Degree: 4, MSE Train: 0.96, MSE Test: 1.04, SE test: [1.4, 1.01, 1.08, 1.07, 0.65] \n"
     ]
    }
   ],
   "source": [
    "for deg in 1:4\n",
    "    @printf(\"Degree: %i, MSE Train: %0.2f, MSE Test: %0.2f, SE test: %s \\n\", \n",
    "        deg, mse_train[deg],  mse_test[deg], round.(gen_error[deg,:], 2))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now recover the averaged parameters for each degree across all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree: 1                    β0, β1 =                        [-2.1, 1.35] \n",
      "Degree: 2                β0, β1, β2 =                [-0.17, 1.13, -1.88] \n",
      "Degree: 3            β0, β1, β2, β3 =           [-0.18, 0.87, -1.88, 0.1] \n",
      "Degree: 4        β0, β1, β2, β3, β4 =    [-0.25, 0.87, -1.67, 0.1, -0.05] \n"
     ]
    }
   ],
   "source": [
    "βpooled = [Array{Float64}(deg + 1) for deg in 1:4]\n",
    "for deg in 1:4\n",
    "    for i in 1:k\n",
    "        βpooled[deg] += βhat_all[i][deg] / k\n",
    "    end\n",
    "end\n",
    "for deg in 1:4\n",
    "   @printf(\"Degree: %i %25.25s = %35.35s \\n\", deg, \n",
    "        join([\"β$i\" for i in 0:deg], \", \"), round.(βpooled[deg], 2))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that for degree 2, the pooled coefficients $\\hat{\\beta}$ do a nice job recovering the true values $$\n",
    "(\\beta_0,\\beta_1, \\beta_2) = (0,1,-2, 0, 0) \\approx (-0.17,1.13,-1.88) = (\\hat{\\beta}_0,\\hat{\\beta}_1, \\hat{\\beta}_2) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    plot(mapslices(mean, gen_error, 2), \n",
    "        seriestype = :bar, ylab = \"mse\", xlab = \"number & color: degree\",\n",
    "        title = \"MSE by degree\",\n",
    "        titlefont = font(12),\n",
    "        guidefont = font(10),\n",
    "        group = 1:4),\n",
    "    plot(transpose(gen_error), \n",
    "        st = :bar, legend = true, xlab = \"color: degree  number: cross-validation sample\",\n",
    "        title = \"Error by degree by cross-validation sample\",\n",
    "        titlefont = font(12),\n",
    "        guidefont = font(10)),\n",
    "    leg = false,\n",
    "    layout = @layout [a{0.3w} b{0.7w}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order = sortperm(x)\n",
    "xo = x[order]\n",
    "Xo = X[order,:]\n",
    "yo = y[order]\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = 1\n",
    "Xdeg = Xo[:,1:(deg + 1)]\n",
    "yhat_folds = hcat([Xdeg*βhat_all[i][deg] for i in 1:k]...)\n",
    "yhat_pooled = Xdeg*βpooled[deg]\n",
    "plot(xo, yo, st = :scatter, alpha = 0.3, ms = 5, \n",
    "    label = \"y\", title = \"Degree 1 fitting\")\n",
    "plot!(xo, yhat_folds, st = :line, lw = 2,\n",
    "    label = hcat([\"yhat fold-$i\" for i in 1:5]...), ls = :dot)\n",
    "plot!(xo, yhat_pooled, st = :line, lw = 2, \n",
    "    alpha = 0.8, label = \"yhat pooled\", color = :black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = 2\n",
    "Xdeg = Xo[:,1:(deg + 1)]\n",
    "yhat_folds = hcat([Xdeg*βhat_all[i][deg] for i in 1:k]...)\n",
    "yhat_pooled = Xdeg*βpooled[deg]\n",
    "plot(xo, yo, st = :scatter, alpha = 0.3, ms = 5, \n",
    "    label = \"y\", title = \"Degree 2 fitting\")\n",
    "plot!(xo, yhat_folds, st = :line, lw = 2, \n",
    "    label = hcat([\"yhat fold-$i\" for i in 1:5]...), ls = :dot)\n",
    "plot!(xo, yhat_pooled, st = :line, lw = 2, alpha = 0.8,\n",
    "    label = \"yhat pooled\", color = :black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = 3\n",
    "Xdeg = Xo[:,1:(deg + 1)]\n",
    "yhat_folds = hcat([Xdeg*βhat_all[i][deg] for i in 1:k]...)\n",
    "yhat_pooled = Xdeg*βpooled[deg]\n",
    "plot(xo, yo, st = :scatter, alpha = 0.3, ms = 5, \n",
    "    label = \"y\", title = \"Degree 3 fitting\")\n",
    "plot!(xo, yhat_folds, st = :line, lw = 2,\n",
    "    label = hcat([\"yhat fold-$i\" for i in 1:5]...), ls = :dot)\n",
    "plot!(xo, yhat_pooled, st = :line, lw = 2, \n",
    "    alpha = 0.8, label = \"yhat pooled\", color = :black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = 4\n",
    "Xdeg = Xo[:,1:(deg + 1)]\n",
    "yhat_folds = hcat([Xdeg*βhat_all[i][deg] for i in 1:k]...)\n",
    "yhat_pooled = Xdeg*βpooled[deg]\n",
    "plot(xo, yo, st = :scatter, alpha = 0.3, ms = 5, \n",
    "    label = \"y\", title = \"Degree 4 fitting\")\n",
    "plot!(xo, yhat_folds, st = :line, lw = 2, \n",
    "    label = hcat([\"yhat fold-$i\" for i in 1:5]...), ls = :dot)\n",
    "plot!(xo, yhat_pooled, st = :line, lw = 2, alpha = 0.8, \n",
    "    label = \"yhat pooled\", color = :black)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
